{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 0. Log in to Galileo!\n",
    "\"\"\"\n",
    "\n",
    "import dataquality\n",
    "\n",
    "dataquality.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50131e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 0.1 Create your first project!\n",
    "\"\"\"\n",
    "\n",
    "dataquality.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84325664",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 0.2 Install some dependencies for this workflow exercise.\n",
    "\"\"\"\n",
    "\n",
    "%pip install torch sklearn transformers pandas numpy pytorch_lightning torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1487f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1.\n",
    "\n",
    "Log your datasets with Galileo.\n",
    "\n",
    "Create the Newsgroup dataset class. Using huggingface Bert Tokenizer.\n",
    "\n",
    "We are introducing some noise to these datasets because \n",
    "the newsgroup dataset is already well labeled.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from transformers import DistilBertTokenizerFast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def introduce_label_errors(df: pd.DataFrame, column: str, shuffle_percent: int) -> pd.DataFrame:\n",
    "    arr = df[column].values\n",
    "    shuffle = np.random.choice(\n",
    "        np.arange(arr.shape[0]), \n",
    "        round(arr.shape[0] * shuffle_percent / 100), \n",
    "        replace=False)\n",
    "    arr[np.sort(shuffle)] = arr[shuffle]\n",
    "    df[column] = arr\n",
    "    return df\n",
    "    \n",
    "\n",
    "class NewsgroupDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str) -> None:\n",
    "        newsgroups = fetch_20newsgroups(subset=\"train\" if split == \"training\" else \"test\", \n",
    "                                        remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.dataset[\"text\"] = newsgroups.data\n",
    "        self.dataset[\"label\"] = newsgroups.target\n",
    "        self.dataset = self.dataset[:23]\n",
    "\n",
    "        # Shuffle some percentage of the training dataset \n",
    "        # to force create mislabeled samples\n",
    "        if split == \"training\":\n",
    "            self.dataset = introduce_label_errors(self.dataset, \"label\", 11)\n",
    "\n",
    "        #\n",
    "        # ðŸ”­ Logging Inputs with Galileo!\n",
    "        #\n",
    "        for i in range(len(self.dataset)):\n",
    "            dataquality.log_input_data({\n",
    "                \"id\": i,\n",
    "                \"text\": self.dataset[\"text\"][i],\n",
    "                \"gold\": str(self.dataset[\"label\"][i]),\n",
    "                \"split\": split})\n",
    "\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.encodings = tokenizer(self.dataset[\"text\"].tolist(), truncation=True, padding=True)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.encodings[\"input_ids\"][idx])\n",
    "        attention_mask = torch.tensor(self.encodings[\"attention_mask\"][idx])\n",
    "        y = self.dataset[\"label\"][idx]\n",
    "        return idx, x, attention_mask, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2.\n",
    "\n",
    "Log model outputs with Galileo.\n",
    "\n",
    "We are using a DistilBERT pytorch lightning class for text classification.\n",
    "\"\"\"\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class LightningDistilBERT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=DistilBertConfig(num_labels=20))\n",
    "        self.feature_extractor = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x, attention_mask, x_idxs, epoch, split):\n",
    "        out = self.model(x, attention_mask=attention_mask)\n",
    "        log_probs = F.log_softmax(out.logits, dim=1)\n",
    "        probs = F.softmax(out.logits, dim=1)\n",
    "        encoded_layers = self.feature_extractor(x, return_dict=False)[0]\n",
    "        if x_idxs is not None:\n",
    "            for i in range(len(x_idxs)):\n",
    "                index = int(x_idxs[i])\n",
    "                prob = probs[i].detach().cpu().numpy().tolist()\n",
    "                emb = encoded_layers[i, 0].detach().cpu().numpy().tolist()\n",
    "                #\n",
    "                # ðŸ”­ Logging outputs with Galileo!\n",
    "                #\n",
    "                dataquality.log_model_output({\n",
    "                    \"id\": int(x_idxs[i]),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"split\": split,\n",
    "                    \"emb\": emb,\n",
    "                    \"prob\": prob,\n",
    "                    \"pred\": str(int(np.argmax(prob)))})\n",
    "        return log_probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Model training step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x=x, attention_mask=attention_mask, x_idxs=x_idxs, epoch=self.current_epoch, split=\"training\")\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.train_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Model validation step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x=x, attention_mask=attention_mask, x_idxs=x_idxs, epoch=self.current_epoch, split=\"validation\")\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.val_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        \"\"\"Model test step.\"\"\"\n",
    "        x_idxs, x, attention_mask, y = batch\n",
    "        log_probs = self(x=x, attention_mask=attention_mask, x_idxs=x_idxs, epoch=self.current_epoch, split=\"test\")\n",
    "        loss = F.nll_loss(log_probs, y)\n",
    "        self.test_acc(torch.argmax(log_probs, 1), y)\n",
    "        self.log(\"test_acc\", self.test_acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Model optimizers.\"\"\"\n",
    "        return torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 3.\n",
    "\n",
    "Instantiate a model and train it with PyTorch Lightning.\n",
    "\"\"\"\n",
    "\n",
    "model = LightningDistilBERT()\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"training\"), batch_size=8, shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"validation\"), batch_size=8, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(NewsgroupDataset(\"test\"), batch_size=8, shuffle=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=2, num_sanity_val_steps=0)\n",
    "\n",
    "trainer.fit(model, train_dataloader, validation_dataloader)\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
