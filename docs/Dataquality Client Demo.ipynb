{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7081078",
   "metadata": {},
   "source": [
    "# Welcome to the dataquality client demo\n",
    "\n",
    "### This will be a brief introduction to how the client works and a bit under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716a0d8",
   "metadata": {},
   "source": [
    "## Installing\n",
    "\n",
    "You can currently install dataquality from pypi\n",
    "`pip install dataquality`\n",
    "\n",
    "But for development, you may want to install it from github. This will give you the latest changes in master\n",
    "\n",
    "`pip install git+https://www.github.com/rungalileo/dataquality.git`\n",
    "\n",
    "You can also clone the repo and install from a path. This is recommended for development\n",
    "\n",
    "`pip install /path/to/dataquality/directory`\n",
    "\n",
    "**(It's good to restart the kernel after an install)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5afe1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q /path/to/dataquality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have cloned the dataquality repo and are running this from the docs folder, you can run this\n",
    "!pip install -q ../dataquality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f0b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or install latest from main\n",
    "!pip install -qqq git+https://www.github.com/rungalileo/dataquality.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c50925",
   "metadata": {},
   "source": [
    "## Components\n",
    "\n",
    "The data quality client is currently very simple. It has just a few components:\n",
    "\n",
    "* logging - the inputs and outputs to your model\n",
    "* config - the urls, usernames, and passwords to interact with the server\n",
    "* init - how you start a new project/run\n",
    "* finish - how you end your run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234111dd",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "To get started, simply `import dataquality`<br>\n",
    "If your environment variables are set, your import will pass through. If not, you will be prompted for some url and config variables.<br>\n",
    "\n",
    "To bypass the prompt, set the following environment variables\n",
    "* `GALILEO_CONSOLE_URL`\n",
    "\n",
    "If you have your server (api, minio, mysql) running locally for development, the following will work\n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ['GALILEO_CONSOLE_URL']=\"http://localhost\"\n",
    "```\n",
    "\n",
    "If you don't set these environment variables, the client will prompt you for the fields (assuming you're running from the newest code).\n",
    "\n",
    "### How do I get everything running locally??\n",
    "\n",
    "See our [CONTRIBUTING](https://github.com/rungalileo/api/blob/main/CONTRIBUTING.md) doc\n",
    "(When running the API, use the `./scripts/run-gunicorn.sh` - you don't need all of them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be42940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['GALILEO_CONSOLE_URL']=\"http://localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dev cluster, run this cell\n",
    "\n",
    "# import os\n",
    "# os.environ['GALILEO_CONSOLE_URL']=\"https://console.dev.rungalileo.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataquality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440db27",
   "metadata": {},
   "source": [
    "## Logging in\n",
    "\n",
    "Once you have dataquality imported, you can log into your server and start logging data<br>\n",
    "\n",
    "To log in, you can call `dataquality.login()` <br>\n",
    "This will prompt you for your auth method, email, and password. You can skip this prompt with the following environment variables:\n",
    "\n",
    "* `GALILEO_USERNAME`\n",
    "* `GALILEO_PASSWORD`\n",
    "\n",
    "### How do I create a user?\n",
    "\n",
    "If you are running everything locally, you can do the following to create the admin user.\n",
    "\n",
    "**Note: If the admin user already exists, you cannot create another one.**\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "data={\n",
    "  \"email\": \"me@rungalileo.io\",\n",
    "  \"first_name\": \"Me\",\n",
    "  \"last_name\": \"Me\",\n",
    "  \"username\": \"Galileo\",\n",
    "  \"auth_method\": \"email\",\n",
    "  \"password\": \"Th3secret_\"\n",
    "}\n",
    "\n",
    "r = requests.post('http://localhost:8088/users/admin', json=data)\n",
    "r.json()\n",
    "```\n",
    "\n",
    "Then set your env vars\n",
    "```\n",
    "import os\n",
    "\n",
    "os.environ[\"GALILEO_USERNAME\"]=\"{r.json()['email']}\"\n",
    "os.environ[\"GALILEO_PASSWORD\"]=\"{r.json()['password']}\"\n",
    "```\n",
    "\n",
    "If you don't set these environment variables, the client will prompt you for the fields (assuming you're running from the newest code).\n",
    "\n",
    "Now login\n",
    "\n",
    "```\n",
    "dataquality.login()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29192d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pwd = \"MyPassword!123\"\n",
    "\n",
    "data={\n",
    "  \"email\": \"me@rungalileo.io\",\n",
    "  \"first_name\": \"Me\",\n",
    "  \"last_name\": \"Me\",\n",
    "  \"username\": \"Galileo\",\n",
    "  \"auth_method\": \"email\",\n",
    "  \"password\": pwd\n",
    "}\n",
    "\n",
    "r = requests.post(f'{dataquality.config.api_url}/users/admin', json=data)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"GALILEO_USERNAME\"]=f\"{r.json()['email']}\"\n",
    "os.environ[\"GALILEO_PASSWORD\"]=pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3224a",
   "metadata": {},
   "source": [
    "## Start my project/run\n",
    "\n",
    "Now you can start using the tool with `dataquality.init()`<br>\n",
    "\n",
    "You **must** provide a `task_type` when calling `init`\n",
    "* A task type describes the kind of modeling you are doing (text classification, multi-label, NER etc).\n",
    "* Currently the only available task is \"text_classification\"\n",
    "\n",
    "You can optionally provide a project name for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.init?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"text_classification\"\n",
    "# Base case\n",
    "dataquality.init(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4bc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New project, unset run (new)\n",
    "dataquality.init(task_type=task, project_name=\"a_new_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a72dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing project, unset run (new)\n",
    "dataquality.init(task_type=task, project_name=\"a_new_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing project, new run\n",
    "dataquality.init(task_type=task, project_name=\"a_new_project\", run_name=\"a_new_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857160d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing project, existing run\n",
    "dataquality.init(task_type=task, project_name=\"a_new_project\", run_name=\"a_new_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New project, new run\n",
    "dataquality.init(task_type=task, project_name=\"a_new_project2\", run_name=\"a_new_run2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb849b7",
   "metadata": {},
   "source": [
    "## Log to my project/run\n",
    "\n",
    "Now that you've started your run, all you need to do is log data to it.<br>\n",
    "\n",
    "All you need to do is call the `dataquality.log_data_input` and `dataquality.log_model_outputs` functions.\n",
    "\n",
    "`dataquality.log_data_input` knows which task you are logging for, and accepts the proper arguments.\n",
    "For \"text_classification\" it is expecting\n",
    "* text - list of strings indicating the text input\n",
    "* label - list of strings indicating the labels\n",
    "* split - string indicating the data split (training, validation, test)\n",
    "* id (optional) - list of ints indicating the id of each row. If not provided, IDs will be added automatically\n",
    "  * NOTE: This ID must match the output ID in log_model_outputs in order to join them for analysis\n",
    "\n",
    "`dataquality.log_model_outputs` also knows which task you are logging for.\n",
    "For \"text_classification\" it is expecting\n",
    "* emb - list of lists of embedding values for a given text input\n",
    "* probs - list of list of probabilities of the confidence per class\n",
    "* split - string indicating the data split (training, validation, test)\n",
    "* epoch - int indicating the training/test/validation epoch for the input\n",
    "* ids - list of ints indicating the matching id to the input row\n",
    "  * NOTE: This ID must match the output ID in log_model_outputs in order to join them for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086f3dd",
   "metadata": {},
   "source": [
    "### log some data\n",
    "\n",
    "We use the `log_input_data` and `log_model_outputs` to log our metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db00ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.init(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "dataset[\"text\"] = newsgroups.data\n",
    "label_ind = newsgroups.target_names\n",
    "dataset[\"label\"] = [label_ind[i] for i in newsgroups.target]\n",
    "dataset = dataset[:100]\n",
    "\n",
    "dataquality.log_input_data(text=dataset['text'], labels=dataset['label'], split=\"train\")\n",
    "dataquality.log_input_data(text=dataset['text'], labels=dataset['label'], split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194d70e",
   "metadata": {},
   "source": [
    "## We validate data before logging\n",
    "\n",
    "#### See what happens with an invalid model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3949c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Labels and text inputs dont match in shape\n",
    "dataquality.log_input_data(text=dataset['text'], labels=dataset['label'][:3], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5560a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate fake model outputs\n",
    "def log_fake_data(log_num: int = 0):\n",
    "    # Ensure unique IDs\n",
    "    # Because we're going to call this twice, we need the other dataset rows for the second call, so /2\n",
    "    num_rows = len(dataset) // 2 \n",
    "        \n",
    "    emb = np.random.rand(num_rows, 800)\n",
    "    prob = np.random.rand(num_rows, 20)\n",
    "    for split in ['test','train']:\n",
    "        epoch = 0\n",
    "        \n",
    "        r = range(num_rows*log_num, num_rows*(log_num+1))\n",
    "        ids = list(r)\n",
    "        dataquality.log_model_outputs(emb=emb, probs=prob, split=split, epoch=epoch, ids=ids)\n",
    "\n",
    "log_fake_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae89fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree .galileo/logs/{dataquality.config.current_project_id}/{dataquality.config.current_run_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af65e13",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "When you call `log_batch_input_data` you are logging the input data for this training job. This would typically be run once (per split).<br>\n",
    "\n",
    "Then, as you train your model in batches, each call to `log_model_outputs` takes the data in that batch, joins it to the input data, and stores it in 3 files, data, emb, and prob.<br>\n",
    "\n",
    "If we were to log another fake dataset to this, we'd see another file in each dir (under the epoch we set).\n",
    "\n",
    "The file names in each subdir will match so we can join them at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fake_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree .galileo/logs/{dataquality.config.current_project_id}/{dataquality.config.current_run_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9d253",
   "metadata": {},
   "source": [
    "## Take a look at our logged model outputs\n",
    "\n",
    "Below is the model output data we've logged to test. You can see all of the values available across both logs<br>\n",
    "To see the training data, just change the variable to `training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "\n",
    "split = \"test\"\n",
    "vaex.open(f'.galileo/logs/{dataquality.config.current_project_id}/{dataquality.config.current_run_id}/{split}/0/*.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48325087",
   "metadata": {},
   "source": [
    "## How do I see my results in the UI?\n",
    "\n",
    "Simply set your labels (`set_labels_for_run`) and call `finish()`\n",
    "\n",
    "Once called, the data will be joined together at a _per-epoch_ level, and added to minio, with one file for each `prob`, `emb`, and `data` per split/epoch. \n",
    "\n",
    "A job will be kicked off to process you data on the server, and after it's done you'll see your results inv the UI\n",
    "\n",
    "#### Why do I need to set my labels?\n",
    "\n",
    "Since your model is simply outputting probabilities, we have no way to map the index of each prediction to the model output. Setting your labels enables us to map them so you can see the meaningful values in the UI.<br>\n",
    "\n",
    "If you have the UI running, you should see it at the URL returned.\n",
    "\n",
    "**Note:** Check out your local API logs to see the background job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc218af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataquality.set_labels_for_run(newsgroups.target_names)\n",
    "dataquality.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d433bde",
   "metadata": {},
   "source": [
    "## That should take ~10-20 seconds to complete (if you are running the server locally)\n",
    "\n",
    "### Now we can export our results to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d27cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataquality.schemas.split import Split\n",
    "from dataquality.clients.api import ApiClient\n",
    "import pandas as pd\n",
    "\n",
    "api_client = ApiClient()\n",
    "pname, rname = api_client.get_project_run_name()\n",
    "api_client.export_run(pname, rname, Split.training, \"training_data.csv\")\n",
    "\n",
    "pd.read_csv(\"training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2fd90",
   "metadata": {},
   "source": [
    "### (Dev only) we can also read the data from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "\n",
    "url = dataquality.config.minio_url\n",
    "client = Minio(url, 'minioadmin', 'minioadmin', secure=(':9000' not in url))\n",
    "p = dataquality.config.current_project_id\n",
    "r = dataquality.config.current_run_id\n",
    "client.fget_object('galileo-project-runs-results', f'{p}/{r}/training/data/data.hdf5', 'training_data.hdf5')\n",
    "client.fget_object('galileo-project-runs-results', f'{p}/{r}/test/data/data.hdf5', 'test_data.hdf5')\n",
    "\n",
    "display(vaex.open('training_data.hdf5'))\n",
    "\n",
    "display(vaex.open('test_data.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a536bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
