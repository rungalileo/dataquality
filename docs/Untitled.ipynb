{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339f4102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from types import ModuleType\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataquality.utils import tqdm\n",
    "\n",
    "\n",
    "CWD = os.getcwd()\n",
    "DATASET = \"newsgroups\"\n",
    "DATASET_NUM_CLASSES = 20\n",
    "BUCKET = \"https://galileo-public-tutorial-data.s3.us-west-1.amazonaws.com\"\n",
    "DATASETS = {\n",
    "    \"training\": f\"{BUCKET}/datasets/original/newsgroups/newsgroups_train.csv\",\n",
    "    \"test\": f\"{BUCKET}/datasets/original/newsgroups/newsgroups_test.csv\",\n",
    "}\n",
    "TASK_TYPE = \"text_classification\"\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 768\n",
    "\n",
    "\n",
    "def download_dataset_from_aws() -> None:\n",
    "    for _, url in DATASETS.items():\n",
    "        fname = os.path.basename(url)\n",
    "        if os.path.exists(fname):  # Only download if dataset isn't present\n",
    "            print(f\"Dataset already exists {fname}\")\n",
    "        urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "\n",
    "def load_dataset_split(split: str) -> pd.DataFrame:\n",
    "    dataset = pd.read_csv(f\"{CWD}/{os.path.basename(DATASETS[split])}\")\n",
    "    print(dataset.info(memory_usage=\"deep\"))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_random_embeddings(batch_size: int, emb_dims: int) -> np.ndarray:\n",
    "    return np.random.rand(batch_size, emb_dims)\n",
    "\n",
    "\n",
    "def generate_random_probabilities(batch_size: int, num_classes: int) -> np.ndarray:\n",
    "    probs = np.random.rand(batch_size, num_classes)\n",
    "    return probs / probs.sum(axis=-1).reshape(-1, 1)  # Normalize to sum to 1\n",
    "\n",
    "\n",
    "def log_data(dataquality: ModuleType, epochs: Optional[int] = NUM_EPOCHS) -> float:\n",
    "    download_dataset_from_aws()\n",
    "    train_dataset = load_dataset_split(\"training\")\n",
    "    test_dataset = load_dataset_split(\"test\")\n",
    "    t_start = time.time()\n",
    "    dataquality.log_input_data(\n",
    "        text=train_dataset[\"text\"],\n",
    "        labels=train_dataset[\"label\"],\n",
    "        ids=train_dataset[\"id\"],\n",
    "        split=\"train\",\n",
    "    )\n",
    "    dataquality.log_input_data(\n",
    "        text=test_dataset[\"text\"],\n",
    "        labels=test_dataset[\"label\"],\n",
    "        ids=test_dataset[\"id\"],\n",
    "        split=\"test\",\n",
    "    )\n",
    "    dataquality.set_labels_for_run(train_dataset[\"label\"].unique())\n",
    "    print(f\"Input logging took {time.time() - t_start} seconds\")\n",
    "    t_start = time.time()\n",
    "    num_classes = train_dataset[\"label\"].nunique()\n",
    "    # Simulates model training loop\n",
    "    for epoch_idx in range(epochs):\n",
    "        print(f\"Epoch {epoch_idx}\")\n",
    "        print(\"Training\")\n",
    "        for i in tqdm(range(0, len(train_dataset), BATCH_SIZE)):\n",
    "            batch = train_dataset[i : i + BATCH_SIZE]\n",
    "            embedding = generate_random_embeddings(len(batch), EMB_DIM)\n",
    "            probs = generate_random_probabilities(len(batch), num_classes)\n",
    "            dataquality.log_model_outputs(\n",
    "                emb=embedding,\n",
    "                probs=probs,\n",
    "                split=\"train\",\n",
    "                epoch=epoch_idx,\n",
    "                ids=batch[\"id\"],\n",
    "            )\n",
    "        print(\"Testing\")\n",
    "        for i in tqdm(range(0, len(test_dataset), BATCH_SIZE)):\n",
    "            batch = test_dataset[i : i + BATCH_SIZE]\n",
    "            embedding = generate_random_embeddings(len(batch), EMB_DIM)\n",
    "            probs = generate_random_probabilities(len(batch), num_classes)\n",
    "            dataquality.log_model_outputs(\n",
    "                emb=embedding,\n",
    "                probs=probs,\n",
    "                split=\"test\",\n",
    "                epoch=epoch_idx,\n",
    "                ids=batch[\"id\"],\n",
    "            )\n",
    "    time_spent = time.time() - t_start\n",
    "    print(f\"Took {time_spent} seconds\")\n",
    "    return time_spent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5fa098",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataquality\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdq\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dq\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      4\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_IT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTASK_TYPE,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m log_data(dataquality\u001b[38;5;241m=\u001b[39mdq, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m dq\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "import dataquality as dq\n",
    "\n",
    "dq.init(\n",
    "    project_name=\"test_IT\",\n",
    "    run_name=f\"{DATASET}_{datetime.today()}\",\n",
    "    task_type=TASK_TYPE,\n",
    ")\n",
    "log_data(dataquality=dq, epochs=1)\n",
    "dq.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176b0784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2022-03-15T13:50:08',\n",
       " 'status': 'finished',\n",
       " 'message': 'finished'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataquality.clients.api import ApiClient\n",
    "\n",
    "c = ApiClient()\n",
    "c.get_run_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344f1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
