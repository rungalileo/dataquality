{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339f4102",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "from types import ModuleType\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataquality.utils import tqdm\n",
    "\n",
    "\n",
    "CWD = os.getcwd()\n",
    "DATASET = \"newsgroups\"\n",
    "DATASET_NUM_CLASSES = 20\n",
    "BUCKET = \"https://galileo-public-tutorial-data.s3.us-west-1.amazonaws.com\"\n",
    "DATASETS = {\n",
    "    \"training\": f\"{BUCKET}/datasets/original/newsgroups/newsgroups_train.csv\",\n",
    "    \"test\": f\"{BUCKET}/datasets/original/newsgroups/newsgroups_test.csv\",\n",
    "}\n",
    "TASK_TYPE = \"text_classification\"\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 32\n",
    "EMB_DIM = 768\n",
    "\n",
    "\n",
    "def download_dataset_from_aws() -> None:\n",
    "    for _, url in DATASETS.items():\n",
    "        fname = os.path.basename(url)\n",
    "        if os.path.exists(fname):  # Only download if dataset isn't present\n",
    "            print(f\"Dataset already exists {fname}\")\n",
    "        urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "\n",
    "def load_dataset_split(split: str) -> pd.DataFrame:\n",
    "    dataset = pd.read_csv(f\"{CWD}/{os.path.basename(DATASETS[split])}\")\n",
    "    print(dataset.info(memory_usage=\"deep\"))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_random_embeddings(batch_size: int, emb_dims: int) -> np.ndarray:\n",
    "    return np.random.rand(batch_size, emb_dims)\n",
    "\n",
    "\n",
    "def generate_random_probabilities(batch_size: int, num_classes: int) -> np.ndarray:\n",
    "    probs = np.random.rand(batch_size, num_classes)\n",
    "    return probs / probs.sum(axis=-1).reshape(-1, 1)  # Normalize to sum to 1\n",
    "\n",
    "\n",
    "def log_data(dataquality: ModuleType, epochs: Optional[int] = NUM_EPOCHS) -> float:\n",
    "    download_dataset_from_aws()\n",
    "    train_dataset = load_dataset_split(\"training\")\n",
    "    test_dataset = load_dataset_split(\"test\")\n",
    "    t_start = time.time()\n",
    "    dataquality.log_input_data(\n",
    "        text=train_dataset[\"text\"],\n",
    "        labels=train_dataset[\"label\"],\n",
    "        ids=train_dataset[\"id\"],\n",
    "        split=\"train\",\n",
    "    )\n",
    "    dataquality.log_input_data(\n",
    "        text=test_dataset[\"text\"],\n",
    "        labels=test_dataset[\"label\"],\n",
    "        ids=test_dataset[\"id\"],\n",
    "        split=\"test\",\n",
    "    )\n",
    "    dataquality.set_labels_for_run(train_dataset[\"label\"].unique())\n",
    "    print(f\"Input logging took {time.time() - t_start} seconds\")\n",
    "    t_start = time.time()\n",
    "    num_classes = train_dataset[\"label\"].nunique()\n",
    "    # Simulates model training loop\n",
    "    for epoch_idx in range(epochs):\n",
    "        print(f\"Epoch {epoch_idx}\")\n",
    "        print(\"Training\")\n",
    "        for i in tqdm(range(0, len(train_dataset), BATCH_SIZE)):\n",
    "            batch = train_dataset[i : i + BATCH_SIZE]\n",
    "            embedding = generate_random_embeddings(len(batch), EMB_DIM)\n",
    "            probs = generate_random_probabilities(len(batch), num_classes)\n",
    "            dataquality.log_model_outputs(\n",
    "                emb=embedding,\n",
    "                probs=probs,\n",
    "                split=\"train\",\n",
    "                epoch=epoch_idx,\n",
    "                ids=batch[\"id\"],\n",
    "            )\n",
    "        print(\"Testing\")\n",
    "        for i in tqdm(range(0, len(test_dataset), BATCH_SIZE)):\n",
    "            batch = test_dataset[i : i + BATCH_SIZE]\n",
    "            embedding = generate_random_embeddings(len(batch), EMB_DIM)\n",
    "            probs = generate_random_probabilities(len(batch), num_classes)\n",
    "            dataquality.log_model_outputs(\n",
    "                emb=embedding,\n",
    "                probs=probs,\n",
    "                split=\"test\",\n",
    "                epoch=epoch_idx,\n",
    "                ids=batch[\"id\"],\n",
    "            )\n",
    "    time_spent = time.time() - t_start\n",
    "    print(f\"Took {time_spent} seconds\")\n",
    "    return time_spent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5fa098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° Retrieving run from existing project, test_IT\n",
      "üèÉ‚Äç‚ôÇÔ∏è Starting run newsgroups_2022-03-15 10:03:58.349892\n",
      "üõ∞ Connected to project, test_IT and created new run, newsgroups_2022-03-15 10:03:58.349892.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11314 entries, 0 to 11313\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      11314 non-null  int64 \n",
      " 1   text    11096 non-null  object\n",
      " 2   label   11314 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 14.6 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7532 entries, 0 to 7531\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      7532 non-null   int64 \n",
      " 1   text    7370 non-null   object\n",
      " 2   label   7532 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.9 MB\n",
      "None\n",
      "Exporting input data [########################################] 100.00% elapsed time  :     0.03s =  0.0m =  0.0h\n",
      "Appending input data [########################################] 100.00% elapsed time  :     0.02s =  0.0m =  0.0h\n",
      " Input logging took 0.4648451805114746 seconds\n",
      "Epoch 0\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3decb7048dc4954ba3bd9927721c357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benepstein/Documents/GitHub/dataquality/.venv/lib/python3.9/site-packages/dataquality/loggers/model_logger/text_classification.py:103: UserWarning: Usage of probs is deprecated, use logits instead\n",
      "  warnings.warn(\"Usage of probs is deprecated, use logits instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f76090ee7ff496bbdc79c714a98bbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.0233378410339355 seconds\n",
      "‚òÅÔ∏è Uploading Data\n",
      "Combining batches for upload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4d1a249eac473e8be561691005aed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1914d0659dd242da80243a91fa21be22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data for upload [########################################] 100.00% elapsed time  :     0.29s =  0.0m =  0.0h\n",
      "Writing data for upload [########################################] 100.00% elapsed time  :     0.17s =  0.0m =  0.0h\n",
      "Writing data for upload [########################################] 100.00% elapsed time  :     0.39s =  0.0m =  0.0h\n",
      " Combining batches for upload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b24c3d21d144a368f02648af5b7f53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aea595ac69348fd8e733ef427003ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data for upload [########################################] 100.00% elapsed time  :     0.17s =  0.0m =  0.0h\n",
      "Writing data for upload [########################################] 100.00% elapsed time  :     0.15s =  0.0m =  0.0h\n",
      "Writing data for upload [########################################] 100.00% elapsed time  :     0.27s =  0.0m =  0.0h\n",
      " üßπ Cleaning up\n",
      "Job default successfully submitted. Results will be available soon at https://console.dev.rungalileo.io/insights?projectId=0b77d7b2-6cf4-4e1d-b1c3-6ab0040aee1a&runId=f4b4646c-f226-4374-bc2d-a344880ae1b4&split=training&taskType=0&activeDepHigh=1&activeDepLow=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_id': '0b77d7b2-6cf4-4e1d-b1c3-6ab0040aee1a',\n",
       " 'run_id': 'f4b4646c-f226-4374-bc2d-a344880ae1b4',\n",
       " 'job_name': 'default',\n",
       " 'labels': ['rec.autos',\n",
       "  'comp.sys.mac.hardware',\n",
       "  'comp.graphics',\n",
       "  'sci.space',\n",
       "  'talk.politics.guns',\n",
       "  'sci.med',\n",
       "  'comp.sys.ibm.pc.hardware',\n",
       "  'comp.os.ms-windows.misc',\n",
       "  'rec.motorcycles',\n",
       "  'talk.religion.misc',\n",
       "  'misc.forsale',\n",
       "  'alt.atheism',\n",
       "  'sci.electronics',\n",
       "  'comp.windows.x',\n",
       "  'rec.sport.hockey',\n",
       "  'rec.sport.baseball',\n",
       "  'soc.religion.christian',\n",
       "  'talk.politics.mideast',\n",
       "  'talk.politics.misc',\n",
       "  'sci.crypt'],\n",
       " 'tasks': None,\n",
       " 'message': 'Processing dataquality!',\n",
       " 'link': 'https://console.dev.rungalileo.io/insights?projectId=0b77d7b2-6cf4-4e1d-b1c3-6ab0040aee1a&runId=f4b4646c-f226-4374-bc2d-a344880ae1b4&split=training&taskType=0&activeDepHigh=1&activeDepLow=0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataquality as dq\n",
    "from datetime import datetime\n",
    "\n",
    "dq.init(\n",
    "    project_name=\"test_IT\",\n",
    "    run_name=f\"{DATASET}_{datetime.today()}\",\n",
    "    task_type=TASK_TYPE,\n",
    ")\n",
    "log_data(dataquality=dq, epochs=1)\n",
    "dq.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176b0784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2022-03-15T14:04:37', 'status': 'started', 'message': 'started'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataquality.clients.api import ApiClient\n",
    "\n",
    "c = ApiClient()\n",
    "c.get_run_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f344f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job...\n",
      "Done!. Job finished with status finished\n"
     ]
    }
   ],
   "source": [
    "c.wait_for_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e73f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
