{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb9305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2140809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting Cell\n",
    "response_template = \"Chatbot:<EOP_TOKEN>\"\n",
    "def create_formatted_prompt(row, idx):\n",
    "    # Assuming completion data\n",
    "    formatted_prompt = f\"\"\"User: {row['summary']}\\nChatbot:<EOP_TOKEN> {row['title']}\"\"\"\n",
    "    return {\"formatted_prompt\": formatted_prompt, \"id\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "30787959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (/Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7140b8f4c346486ebdfd1bce35409d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-7894bddaec6a0cce.arrow\n",
      "Loading cached processed dataset at /Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-117f8b6ca6eccf9b.arrow\n",
      "Loading cached processed dataset at /Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-89a2f792fc523b98.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'title', 'formatted_prompt', 'id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = 100\n",
    "\n",
    "ds = load_dataset(\"billsum\")\n",
    "ds = ds.remove_columns('text')\n",
    "# Add ids\n",
    "ds = ds.map(create_formatted_prompt, with_indices=True)\n",
    "ds_train = Dataset.from_dict(ds['train'][:dataset_size])\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb76642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GenerationConfig, AutoModelForCausalLM\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"Cohere/command-nightly\")\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "# Fake the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4cfda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional, Dict, Union\n",
    "\n",
    "from cohere import AsyncClient\n",
    "from cohere.responses import Generations, StreamingGenerations\n",
    "\n",
    "a_co = AsyncClient('...')\n",
    "\n",
    "async def co_generate(\n",
    "    prompt: Optional[str] = None,\n",
    "    prompt_vars: object = {},\n",
    "    model: Optional[str] = None,\n",
    "    preset: Optional[str] = None,\n",
    "    num_generations: Optional[int] = None,\n",
    "    max_tokens: Optional[int] = None,\n",
    "    temperature: Optional[float] = None,\n",
    "    k: Optional[int] = None,\n",
    "    p: Optional[float] = None,\n",
    "    frequency_penalty: Optional[float] = None,\n",
    "    presence_penalty: Optional[float] = None,\n",
    "    end_sequences: Optional[List[str]] = None,\n",
    "    stop_sequences: Optional[List[str]] = None,\n",
    "    return_likelihoods: Optional[str] = None,\n",
    "    truncate: Optional[str] = None,\n",
    "    logit_bias: Dict[int, float] = {},\n",
    "    raw_prompting: bool = False,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Overwrites `AsyncClient.generate` to we can use the internal `raw_prompting` argument.\n",
    "    TODO: revert back to using `AsyncClient.generate` once `raw_prompting` is added to the SDK.\n",
    "    \"\"\"\n",
    "    json_body = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_vars\": prompt_vars,\n",
    "        \"preset\": preset,\n",
    "        \"num_generations\": num_generations,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"k\": k,\n",
    "        \"p\": p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"end_sequences\": end_sequences,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "        \"return_likelihoods\": return_likelihoods,\n",
    "        \"truncate\": truncate,\n",
    "        \"logit_bias\": logit_bias,\n",
    "        \"stream\": False,\n",
    "        \"raw_prompting\": raw_prompting,\n",
    "    }\n",
    "    response = await a_co._request(cohere.GENERATE_URL, json=json_body, stream=False)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9daf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def a_query_batch(prompts: List[str]) -> torch.Tensor:\n",
    "    response_jobs = []\n",
    "    for prompt in prompts:\n",
    "        # For now append <BOS_TOKEN> and <EOS_TOKEN> NOTE DQ TOKENIZER THINKS IT IS EOP but whatever\n",
    "        prompt = f\"\"\"<BOS_TOKEN>{prompt}<EOS_TOKEN>\"\"\"\n",
    "        response_job = co_generate(\n",
    "            prompt = prompt,\n",
    "            return_likelihoods = \"ALL\",\n",
    "            raw_prompting = True,\n",
    "            max_tokens = 0\n",
    "        )\n",
    "        response_jobs.append(response_job)\n",
    "\n",
    "    responses = await asyncio.gather(*response_jobs) \n",
    "    logprob_responses = []\n",
    "    for response in responses:\n",
    "        logprobs = [token['likelihood'] for token in response['generations'][0]['token_likelihoods']]\n",
    "        logprob_responses.append(torch.Tensor(logprobs))\n",
    "    \n",
    "    # Pad to the max sequence length in the batch\n",
    "    logprob_responses = torch.nn.utils.rnn.pad_sequence(logprob_responses, batch_first=True)\n",
    "    return logprob_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66f64d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangomesselman/Galileo/codebase/dataquality/dataquality/core/__init__.py:27: GalileoWarning: configure is deprecated, use dq.set_console_url and dq.login\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 https://console.dev.rungalileo.io\n",
      "🔭 Logging you into Galileo\n",
      "\n",
      "🚀 You're logged in to Galileo as galileo@rungalileo.io!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GALILEO_CONSOLE_URL']=\"https://console.dev.rungalileo.io\"\n",
    "os.environ[\"GALILEO_USERNAME\"]=\"galileo@rungalileo.io\"\n",
    "os.environ[\"GALILEO_PASSWORD\"]=\"...\"\n",
    "\n",
    "import dataquality as dq\n",
    "from dataquality.integrations.seq2seq.hf import watch\n",
    "dq.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9782b9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Initializing existing public project 'Seq2Seq_DecoderOnly_Cohere'\n",
      "🏃‍♂️ Creating new run '2023-11-14_4'\n",
      "🛰 Connected to existing project 'Seq2Seq_DecoderOnly_Cohere', and new run '2023-11-14_4'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangomesselman/Galileo/codebase/dataquality/dataquality/integrations/seq2seq/hf.py:81: UserWarning: The argument max_target_tokens is only used when working with EncoderDecoder models. This value will be ignored.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "dq.init(\"seq2seq\", project_name=\"Seq2Seq_DecoderOnly_Cohere\")\n",
    "\n",
    "temperature = 0.4\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=15,\n",
    "    # Whether we use multinomial sampling\n",
    "    do_sample=temperature >= 1e-5,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "watch(\n",
    "    model,\n",
    "    fast_tokenizer,\n",
    "    generation_config,\n",
    "    generation_splits=[],\n",
    "    max_input_tokens=1024, # Prompt + Completion\n",
    "    response_template=response_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c16f43ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 100 samples [########################################] 100.00% elapsed time  :     0.00s =  0.0m =  0.0h\n",
      " "
     ]
    }
   ],
   "source": [
    "def log_dataset(ds, input_col=\"summary\", target_col=\"title\", formatted_prompt=\"formatted_prompt\"):\n",
    "    dq.log_dataset(\n",
    "        ds,\n",
    "        text=input_col,\n",
    "        label=target_col,\n",
    "        formatted_prompt=formatted_prompt,\n",
    "        split=\"training\"\n",
    "    )\n",
    "\n",
    "# Log just for training\n",
    "log_dataset(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57c73d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 1\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 2\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 3\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 4\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 5\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 6\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 7\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 8\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n",
      "Processing batch 9\n",
      "Calling up Cohere...\n",
      "DONE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "async def log_model_outputs(ds):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        print (f\"Processing batch {i // batch_size}\")\n",
    "        batch = ds[i: i + batch_size]\n",
    "        batch_ids = batch['id']\n",
    "        batch_model_inputs = batch['formatted_prompt']\n",
    "        \n",
    "        print (\"Calling up Cohere...\")\n",
    "        logprobs = await a_query_batch(batch_model_inputs)\n",
    "        print (\"DONE!\")\n",
    "        print()\n",
    "        \n",
    "        dq.log_model_outputs(\n",
    "            probs = logprobs,\n",
    "            ids = batch_ids,\n",
    "            embs = ... # Shape [bs, emb_dim]\n",
    "        )\n",
    "\n",
    "dq.set_epoch(0)\n",
    "dq.set_split(\"train\")\n",
    "await log_model_outputs(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0610f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️ Uploading Data\n",
      "CuML libraries not found, running standard process. For faster Galileo processing, consider installing\n",
      "`pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5771fa9253374001b9c9f54afbdec7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generation for split training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training (epoch=0):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/348k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x157cfeac0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job default successfully submitted. Results will be available soon at https://console.dev.rungalileo.io/insights/9cf4f816-5f3f-4036-8f27-290d00681111/08cb0ed1-cfd9-4543-89f5-374585d8a781?split=training&taskType=8\n",
      "Waiting for job (you can safely close this window)...\n",
      "\tNo embs found, skipping processing\n",
      "\tNo data embs found, skipping processing\n",
      "Done! Job finished with status completed\n",
      "🧹 Cleaning up\n",
      "🧹 Cleaning up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://console.dev.rungalileo.io/insights/9cf4f816-5f3f-4036-8f27-290d00681111/08cb0ed1-cfd9-4543-89f5-374585d8a781?split=training&taskType=8'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce6d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat data format\n",
    "<BOS_TOKEN>User: t1\n",
    "Chatbot: c1\n",
    "User: t2\n",
    "Chatbot: t2\n",
    "User: t3\n",
    "Chatbot:<EOP_TOKEN> t3<EOS_TOKEN>\n",
    "\n",
    "\n",
    "[\n",
    "    [\"User\", \"text\"],\n",
    "    [\"Chatbot\", \"text\"],\n",
    "    ...\n",
    "] -->\n",
    "[\n",
    "    \"User: text\\nChatbot:\", \"text\",\n",
    "    \"User: text\\nChatbot: text\\nUser: text2\\nChatbot:\", \"text2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1497eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All less than 4k in training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcore",
   "language": "python",
   "name": "mlcore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
