{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef30b15",
   "metadata": {},
   "source": [
    "# Logging an inference run on production data\n",
    "\n",
    "In this notebook we learn how to log an inference run, demonstrating common flows and errors\n",
    "If you are new to the dataquality repo, check out the Dataquality-Client-Demo first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f195a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In this demo we use the same setup as the Dataquality-Client-Demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49761dc-6330-4fca-af6b-1843441947c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GALILEO_CONSOLE_URL']=\"http://localhost:8088\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f76d6-1f13-407c-bad6-28d9e44bd69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have cloned the dataquality repo and are running this from the docs folder, you can run this\n",
    "#!pip install -q ../../../../dataquality\n",
    "import dataquality as dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ed95c",
   "metadata": {},
   "source": [
    "***Create an admin if one doesn't exist. Set admin credentials as environment variables to automatically login during `dataquality.init()` below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce730dc-a8ab-4d32-a07b-659861554ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pwd = \"MyPassword!123\"\n",
    "\n",
    "data={\n",
    "  \"email\": \"me@rungalileo.io\",\n",
    "  \"first_name\": \"Me\",\n",
    "  \"last_name\": \"Me\",\n",
    "  \"username\": \"Galileo\",\n",
    "  \"auth_method\": \"email\",\n",
    "  \"password\": pwd\n",
    "}\n",
    "\n",
    "# This will silently fail with a requests status code of 400 if admin is already set\n",
    "r = requests.post(f'{dq.config.api_url}/users/admin', json=data)\n",
    "\n",
    "import os\n",
    "os.environ[\"GALILEO_USERNAME\"]=\"me@rungalileo.io\"\n",
    "os.environ[\"GALILEO_PASSWORD\"]=pwd\n",
    "dq.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc93a7",
   "metadata": {},
   "source": [
    "We create a few helper functions for creating and logging fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2602e-afd9-4198-a65b-bbd1ff9f5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataset():\n",
    "    newsgroups = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "    dataset = pd.DataFrame()\n",
    "    dataset[\"text\"] = newsgroups.data\n",
    "    label_ind = newsgroups.target_names\n",
    "    dataset[\"label\"] = [label_ind[i] for i in newsgroups.target]\n",
    "    return dataset, label_ind\n",
    "\n",
    "def fetch_dataset(dataset, split, inference_name = None):\n",
    "    if split == \"training\":\n",
    "        return dataset[:100]\n",
    "    if split == \"test\":\n",
    "        return dataset[100:200]\n",
    "\n",
    "    if split == \"inference\":\n",
    "        if inference_name == \"03-14-2022\":\n",
    "            return dataset[200:300]\n",
    "        if inference_name == \"03-21-2022\":\n",
    "            return dataset[300:400]\n",
    "        if inference_name == \"all-customers\":\n",
    "            return dataset[400:500]\n",
    "\n",
    "    raise ValueError(\"Uh oh something happened\")\n",
    "\n",
    "# Generate fake model outputs\n",
    "def log_fake_data(dataset, split):\n",
    "    dataset_len = len(dataset)\n",
    "\n",
    "    emb = np.random.rand(dataset_len, 800)\n",
    "    prob = np.random.rand(dataset_len, 20)\n",
    "    epochs = [0]\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        ids = dataset.index.to_list()\n",
    "        dq.log_model_outputs(embs=emb, probs=prob, split=split, epoch=epoch, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8c6fd-419d-4444-b5f6-4b4ec2515b88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start with a train / test run\n",
    "\n",
    "Inference data will usually be logged after training / test runs. We simulate this flow by populating minio with training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d385523-fb0d-48ce-9c5a-91bd462cab88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dq.init(task_type=\"text_classification\", project_name=\"gonzaga\", run_name=\"duke\")\n",
    "\n",
    "base_dataset, labels = create_dataset()\n",
    "train_dataset = fetch_dataset(base_dataset, \"training\")\n",
    "test_dataset = fetch_dataset(base_dataset, \"test\")\n",
    "\n",
    "dq.log_data_samples(texts=train_dataset['text'], labels=train_dataset['label'], split=\"training\", ids=train_dataset.index.to_list())\n",
    "a = fetch_dataset(base_dataset, \"test\")\n",
    "dq.log_data_samples(texts=test_dataset['text'], labels=test_dataset['label'], split=\"test\", ids=test_dataset.index.to_list())\n",
    "\n",
    "log_fake_data(train_dataset, \"training\")\n",
    "log_fake_data(test_dataset, \"test\")\n",
    "dq.set_labels_for_run(labels)\n",
    "dq.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed792b-a786-4760-b740-9a83d7d9f400",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference run\n",
    "\n",
    "Now log an inference run. Notice that when we log inference data it is appending to Minio, meaning that existing training / test data is not deleted. \n",
    "\n",
    "We can log multiple inference runs with different inference names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3a375-5fde-45ec-addf-657e504919c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dq.init(task_type=\"text_classification\", project_name=\"gonzaga\", run_name=\"duke\")\n",
    "dq.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45d726-0cce-4eae-97f1-e5d39d8ea73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"inference\"\n",
    "INFERENCE_NAMES = [\"03-14-2022\", \"03-21-2022\", \"all-customers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed82d0-17d1-4909-8cd6-40b10cdc09b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset, labels = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b1e8f-6c29-47e2-bbbb-42c64bce94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_dataset = fetch_dataset(base_dataset, split, \"03-14-2022\")\n",
    "week2_dataset = fetch_dataset(base_dataset, split, \"03-21-2022\")\n",
    "all_dataset = fetch_dataset(base_dataset, split, \"all-customers\")\n",
    "datasets = {\n",
    "    \"03-14-2022\": week1_dataset,\n",
    "    \"03-21-2022\": week2_dataset,\n",
    "    \"all-customers\": all_dataset\n",
    "}\n",
    "starting_indices = {\n",
    "    \"03-14-2022\": 200,\n",
    "    \"03-21-2022\": 300,\n",
    "    \"all-customers\": 400\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a2aea-ed58-46b0-ab57-2318c4ccdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inference_name in INFERENCE_NAMES:\n",
    "    starting_index = starting_indices[inference_name]\n",
    "    ids = list(range(starting_index, starting_index + 100))\n",
    "    # Inference doesn't expect labels, but does need an inference name\n",
    "    dq.log_data_samples(\n",
    "        texts=datasets[inference_name][\"text\"],\n",
    "        split=split,\n",
    "        inference_name=inference_name,  # could be removed if we only log 1 inference run at a time, would use stingified timestamp\n",
    "        ids=ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd0769-5ff6-4789-8779-3e943ef884f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_model_outputs(data, starting_index):\n",
    "    num_rows = len(data)\n",
    "    logits = np.random.rand(num_rows, 20) # fake logits\n",
    "    embs = np.random.rand(num_rows, 768) # fake embeddings\n",
    "    ids = list(range(starting_index, starting_index + 100))\n",
    "\n",
    "    return embs, logits, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb0bbf-8d46-4417-9998-b25aeb634735",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inference_name in INFERENCE_NAMES:\n",
    "    # Set split takes in an optional inference name\n",
    "    dq.set_split(split, inference_name=inference_name)\n",
    "\n",
    "    embs, logits, ids = get_model_outputs(datasets[inference_name], starting_indices[inference_name])\n",
    "    dq.log_model_outputs(embs=embs, logits=logits, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d764d8b-409e-4273-909a-653695ca5239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree ~/.galileo/logs/{dq.config.current_project_id}/{dq.config.current_run_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a88aa-ed04-434f-a2d1-e6a09922baeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finish will kickoff job with name \"inference\"\n",
    "dq.set_labels_for_run(labels)\n",
    "dq.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745037d7-ce0e-4ef1-b3d9-3bb4b3b50945",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Log a new training run, inference data is wiped\n",
    "\n",
    "By default, logging a new training or test run wipes all Minio data. We log a new training run and can confirm that all data is wiped in the Minio bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd52b8-fe18-4ed5-bf30-816dc6655ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq.init(task_type=\"text_classification\", project_name=\"gonzaga\", run_name=\"duke\")\n",
    "base_dataset, labels = create_dataset()\n",
    "train_dataset = fetch_dataset(base_dataset, \"training\")\n",
    "dq.log_data_samples(texts=train_dataset['text'], labels=train_dataset['label'], split=\"training\")\n",
    "test_dataset = fetch_dataset(base_dataset, \"test\")\n",
    "dq.log_data_samples(texts=test_dataset['text'], labels=test_dataset['label'], split=\"test\")\n",
    "\n",
    "log_fake_data(len(train_dataset), 1)\n",
    "dq.set_labels_for_run(labels)\n",
    "dq.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316a5ae-f865-45f4-b413-90c77e81a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataquality.schemas.split import Split\n",
    "from dataquality.clients.api import ApiClient\n",
    "import pandas as pd\n",
    "\n",
    "api_client = ApiClient()\n",
    "pname, rname = api_client.get_project_run_name()\n",
    "api_client.export_run(pname, rname, Split.training, \"training_data.csv\")\n",
    "\n",
    "pd.read_csv(\"training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21312a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
