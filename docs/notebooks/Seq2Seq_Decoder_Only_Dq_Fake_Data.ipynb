{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61acdbb2",
   "metadata": {},
   "source": [
    "# Seq2Seq Decoder-Only DQ Test Notebook\n",
    "\n",
    "In this notebook we test the dq client for **DecoderOnly** models using simulated / fake data. The main intention is to battle test the different components of the client without training an actual model - i.e. optimizing for speed!\n",
    "\n",
    "Things that we test:\n",
    "1. Using the watch function - to set the tokenizer + response_template + generation_config \n",
    "2. Logging data (input + target output + formatted prompt) - ensuring\n",
    "   that we properly handle identifying the `response_template` / the \n",
    "   response tokens\n",
    "3. Logging model outputs 1+ epoch - ensuring we strip just the logits\n",
    "   for the response tokens\n",
    "4. Fake model generations - interestingly the best way to do this may be with a small validation dataset + a real LLM model. This depends a bit on design decisions around logging for generation.\n",
    "\n",
    "NOTE: For a first pass we work with just a training dataset\n",
    "\n",
    "Let's get testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7869cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45428158",
   "metadata": {},
   "source": [
    "## Pull data from hf hub\n",
    "\n",
    "Since part of the dq processing involves tokenizing and aligning text / token indices, we work with a small real-world dataset - rather than dummy data.\n",
    "\n",
    "The Billsum dataset contains three columns:\n",
    "\n",
    "<p style=\"text-align: center;\">|| text || summary || title ||</p>\n",
    "\n",
    "We look at just **summary** and **title** and map them as follows:\n",
    "<p style=\"text-align: center;\">(summary, title) --> (input context,  target output)</p>\n",
    "\n",
    "For **DecoderOnly** models we need to specify a formatting function. We use a simple formatting function to create the `formatted_prompt`:\n",
    "```\n",
    "formatted_prompt = f\"\"\"Input: {summary}\\n\\nResponse: {title}\"\"\"\n",
    "```\n",
    "\n",
    "We also use a small subset of the first 100 data rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1c164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"###Response:\"\n",
    "def create_formatted_prompt(row, idx):\n",
    "    formatted_prompt = f\"\"\"###Input: {row['summary']}\\n\\n###Response: {row['title']}\"\"\"\n",
    "    return {\"formatted_prompt\": formatted_prompt, \"id\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866763ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (/Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed8c21a86574f66b901d500e1d4c611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-c6e334700689d4a3.arrow\n",
      "Loading cached processed dataset at /Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-f8df2c4d228909d7.arrow\n",
      "Loading cached processed dataset at /Users/jonathangomesselman/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-985b0a5345b5b56c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'title', 'formatted_prompt', 'id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = 100\n",
    "\n",
    "ds = load_dataset(\"billsum\")\n",
    "ds = ds.remove_columns('text')\n",
    "# Add ids\n",
    "ds = ds.map(create_formatted_prompt, with_indices=True)\n",
    "ds_train = Dataset.from_dict(ds['train'][:100])\n",
    "ds_val = Dataset.from_dict(ds['test'][:100])\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada6a34",
   "metadata": {},
   "source": [
    "## Tokenizing the Data\n",
    "\n",
    "Tokenize the data for use later when faking our logging - i.e. to make sure we log the correct number of logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfb6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GenerationConfig, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5abe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize things\n",
    "def tokenize_formatted_prompts(row):\n",
    "    return tokenizer(row['formatted_prompt'])\n",
    "\n",
    "ds_train = ds_train.map(tokenize_formatted_prompts)\n",
    "ds_val = ds_val.map(tokenize_formatted_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8c6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': \"Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\",\n",
       " 'title': 'A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.',\n",
       " 'formatted_prompt': \"###Input: Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\\n\\n###Response: A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.\",\n",
       " 'id': 0,\n",
       " 'input_ids': [2,\n",
       "  48134,\n",
       "  48214,\n",
       "  35,\n",
       "  18573,\n",
       "  10,\n",
       "  265,\n",
       "  10014,\n",
       "  31,\n",
       "  2366,\n",
       "  9416,\n",
       "  8941,\n",
       "  7,\n",
       "  143,\n",
       "  1356,\n",
       "  50,\n",
       "  744,\n",
       "  14196,\n",
       "  23,\n",
       "  10,\n",
       "  2122,\n",
       "  9,\n",
       "  14,\n",
       "  10014,\n",
       "  11,\n",
       "  2748,\n",
       "  19,\n",
       "  10,\n",
       "  304,\n",
       "  9,\n",
       "  215,\n",
       "  2122,\n",
       "  30,\n",
       "  10,\n",
       "  6651,\n",
       "  1651,\n",
       "  114,\n",
       "  35,\n",
       "  36,\n",
       "  134,\n",
       "  43,\n",
       "  5,\n",
       "  304,\n",
       "  11493,\n",
       "  751,\n",
       "  5,\n",
       "  7401,\n",
       "  9,\n",
       "  265,\n",
       "  9,\n",
       "  5,\n",
       "  265,\n",
       "  10014,\n",
       "  131,\n",
       "  36,\n",
       "  176,\n",
       "  43,\n",
       "  215,\n",
       "  1356,\n",
       "  50,\n",
       "  744,\n",
       "  11493,\n",
       "  148,\n",
       "  10,\n",
       "  675,\n",
       "  14,\n",
       "  215,\n",
       "  2122,\n",
       "  16,\n",
       "  341,\n",
       "  30,\n",
       "  215,\n",
       "  1651,\n",
       "  131,\n",
       "  8,\n",
       "  36,\n",
       "  246,\n",
       "  43,\n",
       "  5,\n",
       "  265,\n",
       "  10014,\n",
       "  8672,\n",
       "  5,\n",
       "  304,\n",
       "  9,\n",
       "  215,\n",
       "  2122,\n",
       "  30,\n",
       "  5,\n",
       "  1651,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  448,\n",
       "  5556,\n",
       "  42,\n",
       "  1783,\n",
       "  11,\n",
       "  3340,\n",
       "  5895,\n",
       "  868,\n",
       "  7,\n",
       "  41,\n",
       "  1356,\n",
       "  50,\n",
       "  744,\n",
       "  14,\n",
       "  775,\n",
       "  31,\n",
       "  41,\n",
       "  1760,\n",
       "  50,\n",
       "  32324,\n",
       "  9,\n",
       "  10,\n",
       "  265,\n",
       "  10014,\n",
       "  14,\n",
       "  21395,\n",
       "  4200,\n",
       "  20037,\n",
       "  50,\n",
       "  18797,\n",
       "  6046,\n",
       "  6,\n",
       "  217,\n",
       "  6046,\n",
       "  14,\n",
       "  35,\n",
       "  36,\n",
       "  134,\n",
       "  43,\n",
       "  21395,\n",
       "  10,\n",
       "  4157,\n",
       "  1846,\n",
       "  50,\n",
       "  10,\n",
       "  1846,\n",
       "  9,\n",
       "  1476,\n",
       "  50,\n",
       "  1760,\n",
       "  9,\n",
       "  758,\n",
       "  4952,\n",
       "  13,\n",
       "  61,\n",
       "  5,\n",
       "  9191,\n",
       "  34,\n",
       "  57,\n",
       "  3828,\n",
       "  11,\n",
       "  143,\n",
       "  461,\n",
       "  131,\n",
       "  50,\n",
       "  36,\n",
       "  176,\n",
       "  43,\n",
       "  6890,\n",
       "  10,\n",
       "  1363,\n",
       "  2970,\n",
       "  13,\n",
       "  61,\n",
       "  5,\n",
       "  9191,\n",
       "  34,\n",
       "  57,\n",
       "  3828,\n",
       "  11,\n",
       "  143,\n",
       "  461,\n",
       "  50,\n",
       "  6046,\n",
       "  13,\n",
       "  61,\n",
       "  5,\n",
       "  9191,\n",
       "  34,\n",
       "  57,\n",
       "  303,\n",
       "  7,\n",
       "  33,\n",
       "  8245,\n",
       "  10,\n",
       "  1853,\n",
       "  50,\n",
       "  331,\n",
       "  2366,\n",
       "  659,\n",
       "  488,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  22763,\n",
       "  15318,\n",
       "  29,\n",
       "  331,\n",
       "  2074,\n",
       "  7,\n",
       "  5,\n",
       "  5239,\n",
       "  14,\n",
       "  215,\n",
       "  2074,\n",
       "  32,\n",
       "  16611,\n",
       "  19,\n",
       "  42,\n",
       "  1783,\n",
       "  6,\n",
       "  4682,\n",
       "  331,\n",
       "  488,\n",
       "  14,\n",
       "  1639,\n",
       "  943,\n",
       "  2591,\n",
       "  31,\n",
       "  9416,\n",
       "  4,\n",
       "  1437,\n",
       "  33534,\n",
       "  10687,\n",
       "  14,\n",
       "  42,\n",
       "  1783,\n",
       "  5658,\n",
       "  45,\n",
       "  28,\n",
       "  37139,\n",
       "  7,\n",
       "  31716,\n",
       "  12820,\n",
       "  143,\n",
       "  1853,\n",
       "  50,\n",
       "  331,\n",
       "  474,\n",
       "  50,\n",
       "  1078,\n",
       "  488,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  448,\n",
       "  5556,\n",
       "  42,\n",
       "  1783,\n",
       "  11,\n",
       "  3340,\n",
       "  5895,\n",
       "  868,\n",
       "  7,\n",
       "  143,\n",
       "  2366,\n",
       "  814,\n",
       "  11,\n",
       "  10,\n",
       "  331,\n",
       "  461,\n",
       "  136,\n",
       "  10,\n",
       "  265,\n",
       "  10014,\n",
       "  11,\n",
       "  61,\n",
       "  70,\n",
       "  1799,\n",
       "  32,\n",
       "  2286,\n",
       "  9,\n",
       "  5,\n",
       "  331,\n",
       "  114,\n",
       "  215,\n",
       "  331,\n",
       "  6,\n",
       "  4319,\n",
       "  42,\n",
       "  1783,\n",
       "  18,\n",
       "  3446,\n",
       "  8,\n",
       "  8200,\n",
       "  117,\n",
       "  97,\n",
       "  6397,\n",
       "  6,\n",
       "  1177,\n",
       "  19170,\n",
       "  10,\n",
       "  16840,\n",
       "  12264,\n",
       "  5,\n",
       "  331,\n",
       "  18,\n",
       "  729,\n",
       "  14,\n",
       "  42,\n",
       "  1783,\n",
       "  5658,\n",
       "  45,\n",
       "  3253,\n",
       "  7,\n",
       "  215,\n",
       "  814,\n",
       "  11,\n",
       "  5,\n",
       "  331,\n",
       "  4,\n",
       "  50118,\n",
       "  50118,\n",
       "  48134,\n",
       "  47806,\n",
       "  35,\n",
       "  83,\n",
       "  1087,\n",
       "  7,\n",
       "  3000,\n",
       "  5,\n",
       "  2366,\n",
       "  9416,\n",
       "  9,\n",
       "  265,\n",
       "  8866,\n",
       "  1976,\n",
       "  304,\n",
       "  9,\n",
       "  2644,\n",
       "  7,\n",
       "  6651,\n",
       "  2665,\n",
       "  4],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b5027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "batch = ds_train[:10]\n",
    "model_inputs = {\n",
    "    'input_ids': batch['input_ids'],\n",
    "    'attention_mask': batch['attention_mask'],\n",
    "    #'labels': batch['input_ids'].copy()\n",
    "}\n",
    "model_inputs = tokenizer.pad(model_inputs, padding=True, return_tensors='pt')\n",
    "model_inputs['labels'] = model_inputs['input_ids'].clone()\n",
    "model_outputs = model(**model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdec3f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 839, 50272])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e2a5755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 838, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.logits[:, :-1].take_along_dim(model_inputs['labels'][:, 1:, None], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a268d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangomesselman/Galileo/codebase/dataquality/dataquality/core/__init__.py:27: GalileoWarning: configure is deprecated, use dq.set_console_url and dq.login\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 https://console.dev.rungalileo.io\n",
      "🔭 Logging you into Galileo\n",
      "\n",
      "🚀 You're logged in to Galileo as galileo@rungalileo.io!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GALILEO_CONSOLE_URL']=\"https://console.dev.rungalileo.io\"\n",
    "os.environ[\"GALILEO_USERNAME\"]=\"galileo@rungalileo.io\"\n",
    "os.environ[\"GALILEO_PASSWORD\"]=\"A11a1una!\"\n",
    "\n",
    "import dataquality as dq\n",
    "from dataquality.integrations.seq2seq.hf import watch\n",
    "dq.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0b23447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Initializing existing public project 'Seq2Seq_DecoderOnly_Log_Logprobs'\n",
      "🏃‍♂️ Creating new run '2023-11-13_9'\n",
      "🛰 Connected to existing project 'Seq2Seq_DecoderOnly_Log_Logprobs', and new run '2023-11-13_9'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangomesselman/Galileo/codebase/dataquality/dataquality/integrations/seq2seq/hf.py:81: UserWarning: The argument max_target_tokens is only used when working with EncoderDecoder models. This value will be ignored.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "dq.init(\"seq2seq\", project_name=\"Seq2Seq_DecoderOnly_Log_Logprobs\")\n",
    "\n",
    "temperature = 0.4\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=15,\n",
    "    # Whether we use multinomial sampling\n",
    "    do_sample=temperature >= 1e-5,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "watch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    generation_config,\n",
    "    generation_splits=[],\n",
    "    max_input_tokens=1024,\n",
    "    response_template=response_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "633bb5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 100 samples [########################################] 100.00% elapsed time  :     0.00s =  0.0m =  0.0h\n",
      " "
     ]
    }
   ],
   "source": [
    "def log_dataset(ds, input_col=\"summary\", target_col=\"title\", formatted_prompt=\"formatted_prompt\"):\n",
    "    dq.log_dataset(\n",
    "        ds,\n",
    "        text=input_col,\n",
    "        label=target_col,\n",
    "        formatted_prompt=formatted_prompt,\n",
    "        split=\"training\"\n",
    "    )\n",
    "\n",
    "# Log just for training\n",
    "log_dataset(ds_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90636aa9",
   "metadata": {},
   "source": [
    "## Logging Model Outputs\n",
    "Log 1 epoch of fake model output data: includes just logits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2aa88ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq len 839\n",
      "Processing batch 0\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 1\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 2\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 3\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 4\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 5\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 6\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 7\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 8\n",
      "Model is working...\n",
      "DONE!\n",
      "\n",
      "Processing batch 9\n",
      "Model is working...\n",
      "DONE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "\n",
    "num_logits = len(tokenizer)\n",
    "batch_size = 10\n",
    "\n",
    "log_logprobs = True\n",
    "\n",
    "@torch.no_grad()\n",
    "def log_epoch(ds):\n",
    "    #ids = ds['id']\n",
    "    max_seq_length = np.max([len(ids) for ids in ds['input_ids']])\n",
    "    print(\"max seq len\", max_seq_length)\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        print (f\"Processing batch {i // batch_size}\")\n",
    "        #batch_ids = ids[i: i + batch_size]\n",
    "        batch = ds[i: i + batch_size]\n",
    "        batch_ids = batch['id']\n",
    "        model_inputs = {\n",
    "            'input_ids': batch['input_ids'],\n",
    "            'attention_mask': batch['attention_mask'],\n",
    "        }\n",
    "        model_inputs = tokenizer.pad(model_inputs, padding=True, return_tensors='pt')\n",
    "        model_inputs['labels'] = model_inputs['input_ids'].clone()\n",
    "        print (\"Model is working...\")\n",
    "        model_outputs = model(**model_inputs)\n",
    "        print (\"DONE!\")\n",
    "        print()\n",
    "        \n",
    "        if log_logprobs:\n",
    "            logprobs = torch.zeros((batch_size, model_outputs.logits.shape[1]))\n",
    "            model_logprobs = torch.nn.functional.log_softmax(model_outputs.logits, dim=-1)\n",
    "            extracted_logprobs = model_logprobs[:, :-1].take_along_dim(model_inputs['labels'][:, 1:, None], dim=-1).squeeze(-1)\n",
    "            logprobs[:, 1:] = extracted_logprobs\n",
    "            dq.log_model_outputs(\n",
    "                probs = logprobs,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "        else:\n",
    "            dq.log_model_outputs(\n",
    "                logits = model_outputs.logits,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "\n",
    "dq.set_epoch(0)\n",
    "dq.set_split(\"train\")\n",
    "log_epoch(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe6e84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️ Uploading Data\n",
      "CuML libraries not found, running standard process. For faster Galileo processing, consider installing\n",
      "`pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06c56e05bac4cc59c064f8563812726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping generation for split training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training (epoch=0):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading data to Galileo:   0%|          | 0.00/342k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job default successfully submitted. Results will be available soon at https://console.dev.rungalileo.io/insights/4551e611-011a-4004-a8ba-b3113980f2ca/2f2fdd25-68ae-4be0-be94-ef62590a7345?split=training&taskType=8\n",
      "Waiting for job (you can safely close this window)...\n",
      "\tDownloading all embedding files for this run\n",
      "\tFinding semantic clusters for training\n",
      "Done! Job finished with status completed\n",
      "🧹 Cleaning up\n",
      "🧹 Cleaning up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://console.dev.rungalileo.io/insights/4551e611-011a-4004-a8ba-b3113980f2ca/2f2fdd25-68ae-4be0-be94-ef62590a7345?split=training&taskType=8'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dq.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f126c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathangomesselman/Galileo/codebase/ml-core/.venv/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 48134, 48214,    35, 18573,    10,   265, 10014,    19,    10,\n",
       "           923,     9,   321,     4,   245,    50,   540,     4,  1437,  1437]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([[2,\n",
    "  48134,\n",
    "  48214,\n",
    "  35,\n",
    "  18573,\n",
    "  10,\n",
    "  265,\n",
    "  10014,\n",
    "]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "053b4f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 48233,     5,   511,  2788,   111,    42,    16,    10,  1296,\n",
       "             4, 48134, 47806,    35]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a little fake sample for generation with decoder only models\n",
    "sample = \"Copy the following text - this is a test.###Response: this is a test.\"\n",
    "response_template = \"###Response:\"\n",
    "response_tokens = [42,    16,    10,  1296,     4]\n",
    "\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\")['input_ids']\n",
    "t_response_template = tokenizer(response_template, add_special_tokens=False)['input_ids']\n",
    "\n",
    "input_prompt = inputs[:, :-len(response_tokens)]\n",
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce2417f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5d75c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=gen_tokens, labels=gen_tokens.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "41000df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 50272])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b03769d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50118, 50118,   133,   511,  2788,   111])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = gen_tokens[0, input_prompt.shape[1]:]\n",
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "862816e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe following text -'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3212b0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [48134, 47806, 35], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(response_template, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc0256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcore",
   "language": "python",
   "name": "mlcore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
