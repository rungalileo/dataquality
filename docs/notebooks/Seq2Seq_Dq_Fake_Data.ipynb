{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9acac185",
   "metadata": {},
   "source": [
    "# Seq2Seq DQ Test Notebook\n",
    "\n",
    "In this notebook we test the dq client for Seq2Seq using simulated / fake data. The main intention is to battle test the different components of the client without training an actual model - i.e. optimizing for speed!\n",
    "\n",
    "Things that we want to test:\n",
    "1. Setting the tokenizer\n",
    "2. Logging data (input + target outputs)\n",
    "3. Logging model outputs 1+ epoch\n",
    "4. Fake model generations - interestingly the best way to do this may be with a small validation dataset + a real LLM model. This depends a bit on design decisions around logging for generation.\n",
    "\n",
    "NOTE: For a first pass we work with just a training dataset\n",
    "\n",
    "Let's get testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6053a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "# import torch\n",
    "\n",
    "# TODO: Add dq import from local"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9074b1ff",
   "metadata": {},
   "source": [
    "## Pull data from hf hub\n",
    "\n",
    "Since part of the dq processing involves tokenizing and aligning text / token indices, we work with a small real-world dataset - rather than dummy data.\n",
    "\n",
    "The Billsum dataset contains three columns:\n",
    "\n",
    "<p style=\"text-align: center;\">|| text || summary || title ||</p>\n",
    "\n",
    "We look at just **summary** and **title** and map them as follows:\n",
    "<p style=\"text-align: center;\">(summary, title) --> (input context,  target output)</p>\n",
    "\n",
    "We also use a small subset of the first 100(0?) data rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435be3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset billsum (/Users/elliottchartock/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f1f0a1b5e24a85864ac74c227dd8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/elliottchartock/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-8163760ca7c203c4.arrow\n",
      "Loading cached processed dataset at /Users/elliottchartock/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-6f832c3394bf0964.arrow\n",
      "Loading cached processed dataset at /Users/elliottchartock/.cache/huggingface/datasets/billsum/default/3.0.0/75cf1719d38d6553aa0e0714c393c74579b083ae6e164b2543684e3e92e0c4cc/cache-fa696985d54ba920.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['summary', 'title', 'id'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = 100\n",
    "\n",
    "ds = load_dataset(\"billsum\")\n",
    "ds = ds.remove_columns('text')\n",
    "# Add ids\n",
    "ds = ds.map(lambda _, idx: {\"id\": idx}, with_indices=True)\n",
    "ds_train = Dataset.from_dict(ds['train'][:100])\n",
    "ds_val = Dataset.from_dict(ds['test'][:100])\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211edbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': \"Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\",\n",
       " 'title': 'A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19ad8b2f",
   "metadata": {},
   "source": [
    "## Logging Data\n",
    "\n",
    "1. Before logging input data log the tokenizer (making sure we use the fast tokenizer)\n",
    "2. Log the input and target output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c78977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6598426daaa74ff58805ecfaed8dff99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80eb8c7ac4204b748b91238547430a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", use_fast=True)\n",
    "\n",
    "# Tokenize things\n",
    "def tokenize_outputs(row):\n",
    "    label_ids = tokenizer(row['title'])['input_ids']\n",
    "    return {'labels': label_ids}\n",
    "\n",
    "ds_train = ds_train.map(tokenize_outputs)\n",
    "ds_val = ds_val.map(tokenize_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855297a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': \"Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\",\n",
       " 'title': 'A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.',\n",
       " 'id': 0,\n",
       " 'labels': [71,\n",
       "  2876,\n",
       "  12,\n",
       "  2006,\n",
       "  8,\n",
       "  3095,\n",
       "  6283,\n",
       "  13,\n",
       "  268,\n",
       "  12311,\n",
       "  1260,\n",
       "  169,\n",
       "  13,\n",
       "  2465,\n",
       "  12,\n",
       "  11069,\n",
       "  2371,\n",
       "  5,\n",
       "  1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4e829c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 17:00:38.947415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0642c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elliottchartock/Code/dataquality/dataquality/core/__init__.py:27: GalileoWarning: configure is deprecated, use dq.set_console_url and dq.login\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° http://localhost:8088\n",
      "üî≠ Logging you into Galileo\n",
      "\n",
      "üöÄ You're logged in to Galileo as user@example.com!\n",
      "‚ú® Initializing new public project 'inquisitive_aquamarine_donkey_96690'\n",
      "üèÉ‚Äç‚ôÇÔ∏è Creating new run '2023-07-18_1'\n",
      "üõ∞ Connected to new project 'inquisitive_aquamarine_donkey_96690', and new run '2023-07-18_1'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GALILEO_CONSOLE_URL']=\"http://localhost:8088\"\n",
    "os.environ[\"GALILEO_USERNAME\"]=\"user@example.com\"\n",
    "os.environ[\"GALILEO_PASSWORD\"]=\"Th3secret_\"\n",
    "\n",
    "import dataquality as dq\n",
    "dq.configure()\n",
    "dq.init(\"seq2seq\")\n",
    "dq.set_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea763af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45d76e55bdc4a2694fd8923d10de055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning characters with tokens:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 100 samples [########################################] 100.00% elapsed time  :     0.00s =  0.0m =  0.0h\n",
      " "
     ]
    }
   ],
   "source": [
    "def log_dataset(ds, input_col=\"summary\", target_output_col=\"title\"):\n",
    "    dq.log_dataset(\n",
    "        ds,\n",
    "        text=input_col,\n",
    "        label=target_output_col,\n",
    "        split=\"training\"\n",
    "    )\n",
    "\n",
    "# Log just for training\n",
    "log_dataset(ds_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94d9fc3a",
   "metadata": {},
   "source": [
    "## Logging Model Outputs\n",
    "Log 1 epoch of fake model output data: includes just logits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523f57e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len ids 100\n",
      "max seq len 111\n"
     ]
    }
   ],
   "source": [
    "num_logits = len(tokenizer)\n",
    "\n",
    "\n",
    "def log_epoch(ds):\n",
    "    ids = ds['id']\n",
    "    max_seq_length = np.max([len(ids) for ids in ds['labels']])\n",
    "    print(\"len ids\", len(ids))\n",
    "    print(\"max seq len\", max_seq_length)\n",
    "    # Shape - [bs, max_seq_len, num_logits]\n",
    "    fake_logits = np.random.randn(len(ids), max_seq_length, num_logits)\n",
    "    dq.log_model_outputs(\n",
    "        logits = fake_logits,\n",
    "        ids = ids\n",
    "    )\n",
    "\n",
    "dq.set_epoch(0)\n",
    "dq.set_split(\"train\")\n",
    "log_epoch(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e363f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Uploading Data\n",
      "CuML libraries not found, running standard process. For faster Galileo processing, consider installing\n",
      "`pip install 'dataquality[cuda]' --extra-index-url=https://pypi.nvidia.com/`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365c23480fba4a1a95d049f9a5af4b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/elliottchartock/Code/dataquality/dataquality/utils/vaex.py\u001b[0m(234)\u001b[0;36mget_output_df\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    232 \u001b[0;31m    \"\"\"\n",
      "\u001b[0m\u001b[0;32m    233 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 234 \u001b[0;31m    \u001b[0mout_frame_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_extension_for_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    235 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    236 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mout_frame_ext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hdf5\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/Users/elliottchartock/Code/dataquality/dataquality/utils/vaex.py\u001b[0m(236)\u001b[0;36mget_output_df\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    234 \u001b[0;31m    \u001b[0mout_frame_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_extension_for_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    235 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 236 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mout_frame_ext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hdf5\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    237 \u001b[0;31m        \u001b[0mout_frame_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dir_name}/{HDF5_STORE}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    238 \u001b[0;31m        \u001b[0;31m# It's possible the files were already concatenated and handled. In that case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "'.arrow'\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'str_cols' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dq\u001b[39m.\u001b[39;49mfinish()\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/utils/helpers.py:27\u001b[0m, in \u001b[0;36mcheck_noop.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m galileo_disabled():\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/core/finish.py:58\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(last_epoch, wait, create_data_embs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# Certain tasks require extra finish logic\u001b[39;00m\n\u001b[1;32m     56\u001b[0m data_logger\u001b[39m.\u001b[39mlogger_config\u001b[39m.\u001b[39mfinish()\n\u001b[0;32m---> 58\u001b[0m data_logger\u001b[39m.\u001b[39;49mupload(last_epoch, create_data_embs\u001b[39m=\u001b[39;49mcreate_data_embs)\n\u001b[1;32m     59\u001b[0m upload_dq_log_file()\n\u001b[1;32m     60\u001b[0m body \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     61\u001b[0m     project_id\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(config\u001b[39m.\u001b[39mcurrent_project_id),\n\u001b[1;32m     62\u001b[0m     run_id\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(config\u001b[39m.\u001b[39mcurrent_run_id),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     feature_names\u001b[39m=\u001b[39mdata_logger\u001b[39m.\u001b[39mlogger_config\u001b[39m.\u001b[39mfeature_names,\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/loggers/data_logger/base_data_logger.py:273\u001b[0m, in \u001b[0;36mBaseGalileoDataLogger.upload\u001b[0;34m(self, last_epoch, create_data_embs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     create_data_embs \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m Split\u001b[39m.\u001b[39mget_valid_attributes():\n\u001b[0;32m--> 273\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupload_split(\n\u001b[1;32m    274\u001b[0m         location, split, object_store, last_epoch, create_data_embs\n\u001b[1;32m    275\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/loggers/data_logger/base_data_logger.py:301\u001b[0m, in \u001b[0;36mBaseGalileoDataLogger.upload_split\u001b[0;34m(self, location, split, object_store, last_epoch, create_data_embs)\u001b[0m\n\u001b[1;32m    299\u001b[0m in_frame_split \u001b[39m=\u001b[39m vaex\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00min_frame_path\u001b[39m}\u001b[39;00m\u001b[39m/*.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mINPUT_DATA_FILE_EXT\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    300\u001b[0m in_frame_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvert_large_string(in_frame_split)\n\u001b[0;32m--> 301\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupload_split_from_in_frame(\n\u001b[1;32m    302\u001b[0m     object_store,\n\u001b[1;32m    303\u001b[0m     in_frame_split,\n\u001b[1;32m    304\u001b[0m     split,\n\u001b[1;32m    305\u001b[0m     split_loc,\n\u001b[1;32m    306\u001b[0m     last_epoch,\n\u001b[1;32m    307\u001b[0m     create_data_embs,\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    309\u001b[0m in_frame_split\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    310\u001b[0m _shutil_rmtree_retry(in_frame_path)\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/loggers/data_logger/base_data_logger.py:400\u001b[0m, in \u001b[0;36mBaseGalileoDataLogger.upload_split_from_in_frame\u001b[0;34m(cls, object_store, in_frame, split, split_loc, last_epoch, create_data_embs)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcreate_and_upload_data_embs(input_batch, split, epoch_or_inf)\n\u001b[1;32m    399\u001b[0m dir_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msplit_loc\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepoch_or_inf\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 400\u001b[0m in_out_frames \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_in_out_frames(\n\u001b[1;32m    401\u001b[0m     input_batch, dir_name, prob_only, split, epoch_or_inf\n\u001b[1;32m    402\u001b[0m )\n\u001b[1;32m    403\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mupload_in_out_frames(object_store, in_out_frames, split, epoch_or_inf)\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/loggers/data_logger/base_data_logger.py:428\u001b[0m, in \u001b[0;36mBaseGalileoDataLogger.create_in_out_frames\u001b[0;34m(cls, in_frame, dir_name, prob_only, split, epoch_or_inf)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_in_out_frames\u001b[39m(\n\u001b[1;32m    407\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m     epoch_or_inf: Union[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[1;32m    413\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseLoggerDataFrames:\n\u001b[1;32m    414\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Formats the input data and model output data\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[39m    In this step, we concatenate the many hdf5 files created during model training\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39m    :param epoch_or_inf: The epoch or inference name we are logging for\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     out_frame \u001b[39m=\u001b[39m get_output_df(\n\u001b[1;32m    429\u001b[0m         dir_name, prob_only, split, epoch_or_inf\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    431\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpdb\u001b[39;00m; pdb\u001b[39m.\u001b[39mset_trace()\n\u001b[1;32m    432\u001b[0m     epoch_or_inf_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minference_name\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m Split\u001b[39m.\u001b[39minference \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Code/dataquality/dataquality/utils/vaex.py:236\u001b[0m, in \u001b[0;36mget_output_df\u001b[0;34m(dir_name, prob_only, split, epoch_or_inf)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdb\u001b[39;00m; pdb\u001b[39m.\u001b[39mset_trace()\n\u001b[1;32m    234\u001b[0m out_frame_ext \u001b[39m=\u001b[39m get_extension_for_dir(dir_name)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mif\u001b[39;00m out_frame_ext \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.hdf5\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     out_frame_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdir_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mHDF5_STORE\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# It's possible the files were already concatenated and handled. In that case\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39m# just open the processed file\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'str_cols' referenced before assignment"
     ]
    }
   ],
   "source": [
    "dq.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f766c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a255b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
